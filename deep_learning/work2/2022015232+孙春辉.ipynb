{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b3e0a72",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "### 作业二\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ece50dcd",
   "metadata": {},
   "source": [
    "1. 在`linear-regression-concise.ipynb`例子的基础,如果我们将权重初始化为零，会发生什么。算法仍然有效吗？试分析原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2cb7ec-b9b3-4487-bd9f-7c2a1b61aff4",
   "metadata": {},
   "source": [
    "answer：\n",
    "\n",
    "线性回归模型中，其训练后的值和标准值进行计算损失函数，再根据插值，反向传播这样就可以更迭线性回归函数的学习参数，在更迭了学习参数了之后，其值就不再为零，就可以再次多次更迭了。线性回归模型的凸性保证了权重零初始化的可行性，算法依然有效。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2606d91",
   "metadata": {},
   "source": [
    "2. 在`linear-regression-concise.ipynb`例子的基础上, 尝试调整超参数，例如批量大小、迭代周期数和学习率，观察损失函数值下降的快慢。试分析原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b0495-dae5-45cc-a737-097caee3bcc8",
   "metadata": {},
   "source": [
    "1、更改批量大小，将批量大小变大则损失函数下降变慢，波动变小，原因是大批量计算梯度更加准确，但是更新频率降低，每次迭代计算的是批量样本个数的平均梯度，所以跟稳定也更准确但是更新较慢，反之则是更新快，但是准确度和稳定性较为差。\n",
    "\n",
    "2、更改学习率，降低学习率损失函数下降缓慢但是稳定，原因是小步长训练更加稳定，但是需要多次训练才能够收敛；反之则会快速下降并且有可能会快速下降，但是可能由于下降数值过大而导致无法收敛导致发散，产生梯度震荡，最终的测量结果不准确。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0491b3fb",
   "metadata": {},
   "source": [
    "3. 如果样本个数不能被批量大小整除，`linear-regression-scratch.ipynb`中`data_iter`函数的行为会有什么变化？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2453f8-436b-421b-951d-b5d601d4968d",
   "metadata": {},
   "source": [
    "由于不能够整除，导致有些组别的样本数量和其他的样本数量是不同的，会导致数据类型特征不一致。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "512996f2",
   "metadata": {},
   "source": [
    "4. 用Huber损失代替Fsion-Mnist分类中的原损失函数MSE，即\n",
    "    $$l(y,y') = \\begin{cases}|y-y'| -\\frac{\\sigma}{2} & \\text{ if } |y-y'| > \\sigma \\\\ \\frac{1}{2 \\sigma} (y-y')^2 & \\text{ 其它情况}\\end{cases}$$ \n",
    "    $\\sigma$为`学号后两位`除以100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bea505-687b-407f-b68b-e73390f979c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 4.205307\n",
      "epoch 2, loss 3.784645\n",
      "epoch 3, loss 3.370496\n",
      "epoch 4, loss 2.961431\n",
      "epoch 5, loss 2.556453\n",
      "epoch 6, loss 2.155634\n",
      "epoch 7, loss 1.760211\n",
      "epoch 8, loss 1.370105\n",
      "epoch 9, loss 0.986319\n",
      "epoch 10, loss 0.612041\n",
      "epoch 11, loss 0.256485\n",
      "epoch 12, loss 0.016687\n",
      "epoch 13, loss 0.000336\n",
      "epoch 14, loss 0.000206\n",
      "epoch 15, loss 0.000205\n",
      "epoch 16, loss 0.000205\n",
      "epoch 17, loss 0.000205\n",
      "epoch 18, loss 0.000205\n",
      "epoch 19, loss 0.000205\n",
      "epoch 20, loss 0.000205\n",
      "epoch 21, loss 0.000205\n",
      "epoch 22, loss 0.000205\n",
      "epoch 23, loss 0.000205\n",
      "epoch 24, loss 0.000205\n",
      "epoch 25, loss 0.000205\n",
      "epoch 26, loss 0.000205\n",
      "epoch 27, loss 0.000205\n",
      "epoch 28, loss 0.000206\n",
      "epoch 29, loss 0.000205\n",
      "epoch 30, loss 0.000206\n",
      "w的估计误差: tensor([-1.2469e-04, -3.3379e-05])\n",
      "b的估计误差: tensor([-0.0003])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# 生成数据\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 50\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "\n",
    "# 定义模型\n",
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2, 1))\n",
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)\n",
    "\n",
    "# 自定义Huber损失 (σ=0.25)\n",
    "class HuberLoss(nn.Module):\n",
    "    def __init__(self, sigma=0.25):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        diff = torch.abs(y_pred - y_true)\n",
    "        quadratic = torch.min(diff, torch.tensor(self.sigma))\n",
    "        linear = diff - quadratic\n",
    "        return (0.5 * quadratic**2 / self.sigma + linear).mean()\n",
    "\n",
    "loss = HuberLoss(sigma=0.25)  # 使用Huber损失替代MSE\n",
    "\n",
    "# 训练设置\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        trainer.zero_grad()\n",
    "        l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')\n",
    "\n",
    "# 输出结果\n",
    "w = net[0].weight.data\n",
    "print('w的估计误差:', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('b的估计误差:', true_b - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f66bdc-c614-476d-8166-a78fc76a1f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "required_libs": [],
  "vscode": {
   "interpreter": {
    "hash": "50bfc351199d956b4024fbbd9aca69be4a9c56b71e449cee70b952cf93a37264"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
