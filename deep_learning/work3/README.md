# 多层感知机(MLP)实验

这个项目包含了一系列实验，用于探索多层感知机(MLP)的不同配置和参数设置对模型性能的影响。这些实验基于`mlp-concise.ipynb`文件中的模型，并进行了扩展和优化。

## 实验内容

本项目包含三个主要实验：

1. **不同隐藏层数量和隐藏单元数量**
   - 测试了1层到4层隐藏层的不同配置
   - 尝试了不同的隐藏单元数量（128, 256等）
   - 测试了不同的学习率（0.01, 0.05, 0.1）

2. **不同激活函数**
   - 测试了ReLU、Sigmoid、Tanh、LeakyReLU和ELU等激活函数
   - 比较了它们在相同网络结构下的性能

3. **不同权重初始化方法**
   - 测试了Normal(0.01)、Normal(0.1)、Xavier、Kaiming和Zeros等初始化方法
   - 分析了不同初始化方法对模型收敛速度和最终性能的影响

## 如何运行

1. 确保已安装所需的依赖包：
   ```
   pip install torch matplotlib d2l
   ```

2. 运行实验脚本：
   ```
   python mlp_experiments.py
   ```

3. 查看结果：
   - 实验结果将打印到控制台
   - 训练过程的图表将保存为PNG文件

## 实验结果分析

### 隐藏层数量和隐藏单元数量

通过实验，我们可以发现：
- 对于Fashion-MNIST数据集，2-3层隐藏层通常能提供较好的性能
- 隐藏单元数量应该随着层数的增加而逐渐减少
- 学习率对模型性能有显著影响，需要根据网络深度适当调整

### 激活函数

不同激活函数的性能比较：
- ReLU通常是最佳选择，因为它能有效缓解梯度消失问题
- LeakyReLU在某些情况下可能比标准ReLU表现更好
- Sigmoid和Tanh在深层网络中容易导致梯度消失

### 权重初始化

不同初始化方法的比较：
- Xavier和Kaiming初始化通常比简单的正态分布初始化效果更好
- 对于使用ReLU激活函数的网络，Kaiming初始化特别有效
- 零初始化会导致网络无法学习，应避免使用

## 最佳实践建议

基于实验结果，我们推荐以下配置：

1. **网络结构**：使用2-3层隐藏层，隐藏单元数量逐层减少（如256->128->64）
2. **激活函数**：使用ReLU或LeakyReLU
3. **权重初始化**：使用Kaiming初始化（对于ReLU激活函数）或Xavier初始化
4. **学习率**：对于较深的网络，使用较小的学习率（如0.01）

这些配置能够在Fashion-MNIST数据集上获得较好的分类性能。 