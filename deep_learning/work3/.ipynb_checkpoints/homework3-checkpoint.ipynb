{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5041f47",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "### 作业三\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11bf5e47",
   "metadata": {},
   "source": [
    "#### 一、以`softmax-regression-concise.ipynb`为例, 完成下列问题。\n",
    "1. `net`的输入`x`和输出`y_hat`的维度是什么？\n",
    "2. 尝试调整超参数，例如批量大小、迭代周期数和学习率，并查看结果。\n",
    "3. 增加迭代周期的数量。为什么测试精度会在一段时间后降低？有什么办法可以解决这个问题？\n",
    "4. 将输入`x`的宽和高调整为 $64 \\times 64$, 查看试验结果。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae611783-e9c0-4d2d-898b-0ad18dfc7c50",
   "metadata": {},
   "source": [
    "1、![image](./shape.jpg)  \n",
    "\n",
    "the x's shape is [batch , 1, 28, 28]\n",
    "the y's shape is [batch_size , 10]\n",
    "\n",
    "\n",
    "3、 可以通过在测试集合其精度不再提升的时候提前停止训练，也可以通过L2正则化的方式进行权重衰减的方式稳定其精确度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37f46f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment: Default\n",
      "Epoch 1/10:\n",
      "  Training loss: 0.6885\n",
      "  Validation loss: 0.5741\n",
      "Epoch 2/10:\n",
      "  Training loss: 0.5220\n",
      "  Validation loss: 0.5331\n",
      "Epoch 3/10:\n",
      "  Training loss: 0.4870\n",
      "  Validation loss: 0.5155\n",
      "Epoch 4/10:\n",
      "  Training loss: 0.4687\n",
      "  Validation loss: 0.4871\n"
     ]
    }
   ],
   "source": [
    "# 第二个问题:\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(batch_size, num_epochs, learning_rate):\n",
    "    # Load data\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    \n",
    "    # Define model\n",
    "    net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n",
    "    \n",
    "    # Initialize weights\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.normal_(m.weight, std=0.01)\n",
    "    \n",
    "    net.apply(init_weights)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss_sum = 0\n",
    "        train_batch_count = 0\n",
    "        \n",
    "        for x, y in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(x)\n",
    "            loss = loss_func(y_hat, y)\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "            train_loss_sum += loss.mean().detach().numpy()\n",
    "            train_batch_count += 1\n",
    "        \n",
    "        avg_train_loss = train_loss_sum / train_batch_count\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        test_loss_sum = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for x_val, y_val in test_iter:\n",
    "            y_pre = net(x_val)\n",
    "            loss_val = loss_func(y_pre, y_val)\n",
    "            test_loss_sum += loss_val.mean().detach().numpy()\n",
    "            test_batch_count += 1\n",
    "        \n",
    "        avg_test_loss = test_loss_sum / test_batch_count\n",
    "        test_losses.append(avg_test_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Training loss: {avg_train_loss:.4f}')\n",
    "        print(f'  Validation loss: {avg_test_loss:.4f}')\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "# Experiment with different hyperparameters\n",
    "experiments = [\n",
    "    {'batch_size': 128, 'num_epochs': 10, 'learning_rate': 0.1, 'label': 'Default'},\n",
    "    {'batch_size': 64, 'num_epochs': 10, 'learning_rate': 0.1, 'label': 'Smaller batch'},\n",
    "    {'batch_size': 256, 'num_epochs': 10, 'learning_rate': 0.1, 'label': 'Larger batch'},\n",
    "    {'batch_size': 256, 'num_epochs': 10, 'learning_rate': 0.01, 'label': 'Lower learning rate'},\n",
    "    {'batch_size': 256, 'num_epochs': 10, 'learning_rate': 0.5, 'label': 'Higher learning rate'}\n",
    "]\n",
    "\n",
    "# Run experiments and plot results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\nRunning experiment: {exp['label']}\")\n",
    "    train_losses, test_losses = train_model(\n",
    "        exp['batch_size'], \n",
    "        exp['num_epochs'], \n",
    "        exp['learning_rate']\n",
    "    )\n",
    "    \n",
    "    plt.plot(train_losses, label=f\"{exp['label']} - Train\")\n",
    "    plt.plot(test_losses, label=f\"{exp['label']} - Test\", linestyle='--')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss for Different Hyperparameters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('hyperparameter_experiments.png')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24ce01-2705-4f36-86a9-e237df6c885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第四个问题的解答\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 自定义数据集类，用于调整图像大小\n",
    "class ResizedFashionMNIST:\n",
    "    def __init__(self, root, train, download, transform, target_transform=None):\n",
    "        self.dataset = FashionMNIST(root=root, train=train, download=download, \n",
    "                                    transform=transform, target_transform=target_transform)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# 加载调整大小后的数据\n",
    "def load_resized_data(batch_size, image_size=64):\n",
    "    # 定义转换\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),  # 调整图像大小为64×64\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # 加载训练集\n",
    "    train_dataset = ResizedFashionMNIST(\n",
    "        root='./data', \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 加载测试集\n",
    "    test_dataset = ResizedFashionMNIST(\n",
    "        root='./data', \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_iter, test_iter\n",
    "\n",
    "def train_model(batch_size, num_epochs, learning_rate, image_size=64):\n",
    "    # 加载调整大小后的数据\n",
    "    train_iter, test_iter = load_resized_data(batch_size, image_size)\n",
    "    \n",
    "    # 定义模型 - 注意输入维度变化\n",
    "    input_dim = image_size * image_size  # 64×64 = 4096\n",
    "    net = nn.Sequential(nn.Flatten(), nn.Linear(input_dim, 10))\n",
    "    \n",
    "    # 初始化权重\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.normal_(m.weight, std=0.01)\n",
    "    \n",
    "    net.apply(init_weights)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 训练\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练\n",
    "        train_loss_sum = 0\n",
    "        train_batch_count = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for x, y in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(x)\n",
    "            loss = loss_func(y_hat, y)\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "            train_loss_sum += loss.mean().detach().numpy()\n",
    "            train_batch_count += 1\n",
    "            \n",
    "            # 计算训练准确率\n",
    "            _, predicted = torch.max(y_hat.data, 1)\n",
    "            train_total += y.size(0)\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "        \n",
    "        avg_train_loss = train_loss_sum / train_batch_count\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # 验证\n",
    "        test_loss_sum = 0\n",
    "        test_batch_count = 0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        for x_val, y_val in test_iter:\n",
    "            y_pre = net(x_val)\n",
    "            loss_val = loss_func(y_pre, y_val)\n",
    "            test_loss_sum += loss_val.mean().detach().numpy()\n",
    "            test_batch_count += 1\n",
    "            \n",
    "            # 计算测试准确率\n",
    "            _, predicted = torch.max(y_pre.data, 1)\n",
    "            test_total += y_val.size(0)\n",
    "            test_correct += (predicted == y_val).sum().item()\n",
    "        \n",
    "        avg_test_loss = test_loss_sum / test_batch_count\n",
    "        test_losses.append(avg_test_loss)\n",
    "        test_accuracy = 100 * test_correct / test_total\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  训练损失: {avg_train_loss:.4f}, 训练准确率: {train_accuracy:.2f}%')\n",
    "        print(f'  验证损失: {avg_test_loss:.4f}, 验证准确率: {test_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# 实验1：使用原始大小 (28×28)\n",
    "print(\"\\n实验1：使用原始大小 (28×28)\")\n",
    "train_losses1, test_losses1, train_acc1, test_acc1 = train_model(\n",
    "    batch_size=256, \n",
    "    num_epochs=10, \n",
    "    learning_rate=0.1,\n",
    "    image_size=28\n",
    ")\n",
    "\n",
    "# 实验2：使用调整后的大小 (64×64)\n",
    "print(\"\\n实验2：使用调整后的大小 (64×64)\")\n",
    "train_losses2, test_losses2, train_acc2, test_acc2 = train_model(\n",
    "    batch_size=256, \n",
    "    num_epochs=10, \n",
    "    learning_rate=0.1,\n",
    "    image_size=64\n",
    ")\n",
    "\n",
    "# 绘制结果\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 绘制损失\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_losses1, label='训练损失')\n",
    "plt.plot(test_losses1, label='验证损失')\n",
    "plt.xlabel('迭代周期')\n",
    "plt.ylabel('损失')\n",
    "plt.title('实验1：原始大小 (28×28)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train_losses2, label='训练损失')\n",
    "plt.plot(test_losses2, label='验证损失')\n",
    "plt.xlabel('迭代周期')\n",
    "plt.ylabel('损失')\n",
    "plt.title('实验2：调整后大小 (64×64)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 绘制准确率\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_acc1, label='训练准确率')\n",
    "plt.plot(test_acc1, label='验证准确率')\n",
    "plt.xlabel('迭代周期')\n",
    "plt.ylabel('准确率 (%)')\n",
    "plt.title('实验1：原始大小 (28×28)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_acc2, label='训练准确率')\n",
    "plt.plot(test_acc2, label='验证准确率')\n",
    "plt.xlabel('迭代周期')\n",
    "plt.ylabel('准确率 (%)')\n",
    "plt.title('实验2：调整后大小 (64×64)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('resize_experiments.png')\n",
    "plt.show()\n",
    "\n",
    "# 比较结果\n",
    "print(\"\\n结果比较：\")\n",
    "print(f\"原始大小 (28×28) - 最终训练准确率: {train_acc1[-1]:.2f}%, 最终验证准确率: {test_acc1[-1]:.2f}%\")\n",
    "print(f\"调整后大小 (64×64) - 最终训练准确率: {train_acc2[-1]:.2f}%, 最终验证准确率: {test_acc2[-1]:.2f}%\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79d1a272",
   "metadata": {},
   "source": [
    "#### 二、在`mlp-consice.ipynb`的基础上，完成下列问题。\n",
    "1. 尝试添加不同数量的隐藏层（也可以修改学习率），怎么样设置效果最好？\n",
    "1. 尝试不同的激活函数，哪个效果最好？\n",
    "1. 尝试不同的方案来初始化权重，什么方法效果最好？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3023e9d",
   "metadata": {},
   "source": [
    "##### 三、将`underfit-overfit.ipynb`中的神经网络`net`通过继承`nn.Modual`的方式改为一个包含三个隐藏层(权重层)的MLP,其中激活函数为`ReLU()`,观察测试结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed7b66-c7e4-4851-b3b7-6c352113f70e",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63167219",
   "metadata": {},
   "source": [
    "##### 四、在`weight-decay.ipynb`, PyTorch构建$L_1$正则项$||w_i||_{1}$, 与$L_2$正则化相比，$L_1$正则化的试验结果有何不同？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbe0fb7a",
   "metadata": {},
   "source": [
    "##### 五、以`dropout.ipynb`中的神经网络`Net`为例，完成下列问题。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f564abd",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "1. 如果更改第一层和第二层的丢弃概率，会发生什么情况？具体地说，如果交换这两个层，会发生什么情况？设计一个实验来回答这些问题，定量描述该结果，并总结定性的结论。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15f9280e",
   "metadata": {},
   "source": [
    "2. 增加训练轮数，并将使用丢弃法和不使用丢弃法时获得的结果进行比较。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "required_libs": [],
  "vscode": {
   "interpreter": {
    "hash": "807cc6a481f25a5b222cd58a3949a299ed2f2634387eaba6775d5c01f71eeb59"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
