{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb495041",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 门控循环单元（GRU）\n",
    ":label:`sec_gru`\n",
    "\n",
    "在 :numref:`sec_bptt`中，\n",
    "我们讨论了如何在循环神经网络中计算梯度，\n",
    "以及矩阵连续乘积可以导致梯度消失或梯度爆炸的问题。\n",
    "下面我们简单思考一下这种梯度异常在实践中的意义：\n",
    "\n",
    "* 我们可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。\n",
    "  考虑一个极端情况，其中第一个观测值包含一个校验和，\n",
    "  目标是在序列的末尾辨别校验和是否正确。\n",
    "  在这种情况下，第一个词元的影响至关重要。\n",
    "  我们希望有某些机制能够在一个记忆元里存储重要的早期信息。\n",
    "  如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度，\n",
    "  因为它会影响所有后续的观测值。\n",
    "* 我们可能会遇到这样的情况：一些词元没有相关的观测值。\n",
    "  例如，在对网页内容进行情感分析时，\n",
    "  可能有一些辅助HTML代码与网页传达的情绪无关。\n",
    "  我们希望有一些机制来*跳过*隐状态表示中的此类词元。\n",
    "* 我们可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。\n",
    "  例如，书的章节之间可能会有过渡存在，\n",
    "  或者证券的熊市和牛市之间可能会有过渡存在。\n",
    "  在这种情况下，最好有一种方法来*重置*我们的内部状态表示。\n",
    "\n",
    "在学术界已经提出了许多方法来解决这类问题。\n",
    "其中最早的方法是\"长短期记忆\"（long-short-term memory，LSTM）\n",
    " :cite:`Hochreiter.Schmidhuber.1997`，\n",
    "我们将在 :numref:`sec_lstm`中讨论。\n",
    "门控循环单元（gated recurrent unit，GRU）\n",
    " :cite:`Cho.Van-Merrienboer.Bahdanau.ea.2014`\n",
    "是一个稍微简化的变体，通常能够提供同等的效果，\n",
    "并且计算 :cite:`Chung.Gulcehre.Cho.ea.2014`的速度明显更快。\n",
    "由于门控循环单元更简单，我们从它开始解读。\n",
    "\n",
    "## 门控隐状态\n",
    "\n",
    "门控循环单元与普通的循环神经网络之间的关键区别在于：\n",
    "前者支持隐状态的门控。\n",
    "这意味着模型有专门的机制来确定应该何时更新隐状态，\n",
    "以及应该何时重置隐状态。\n",
    "这些机制是可学习的，并且能够解决了上面列出的问题。\n",
    "例如，如果第一个词元非常重要，\n",
    "模型将学会在第一次观测之后不更新隐状态。\n",
    "同样，模型也可以学会跳过不相关的临时观测。\n",
    "最后，模型还将学会在需要的时候重置隐状态。\n",
    "下面我们将详细讨论各类门控。\n",
    "\n",
    "### 重置门和更新门\n",
    "\n",
    "我们首先介绍*重置门*（reset gate）和*更新门*（update gate）。\n",
    "我们把它们设计成$(0, 1)$区间中的向量，\n",
    "这样我们就可以进行凸组合。\n",
    "重置门允许我们控制“可能还想记住”的过去状态的数量；\n",
    "更新门将允许我们控制新状态中有多少个是旧状态的副本。\n",
    "\n",
    "我们从构造这些门控开始。 :numref:`fig_gru_1`\n",
    "描述了门控循环单元中的重置门和更新门的输入，\n",
    "输入是由当前时间步的输入和前一时间步的隐状态给出。\n",
    "两个门的输出是由使用sigmoid激活函数的两个全连接层给出。\n",
    "\n",
    "![在门控循环单元模型中计算重置门和更新门](../img/gru-1.svg)\n",
    ":label:`fig_gru_1`\n",
    "\n",
    "我们来看一下门控循环单元的数学表达。\n",
    "对于给定的时间步$t$，假设输入是一个小批量\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$\n",
    "（样本个数$n$，输入个数$d$），\n",
    "上一个时间步的隐状态是\n",
    "$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$\n",
    "（隐藏单元个数$h$）。\n",
    "那么，重置门$\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}$和\n",
    "更新门$\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}$的计算如下所示：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r),\\\\\n",
    "\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$\\mathbf{W}_{xr}, \\mathbf{W}_{xz} \\in \\mathbb{R}^{d \\times h}$\n",
    "和$\\mathbf{W}_{hr}, \\mathbf{W}_{hz} \\in \\mathbb{R}^{h \\times h}$是权重参数，\n",
    "$\\mathbf{b}_r, \\mathbf{b}_z \\in \\mathbb{R}^{1 \\times h}$是偏置参数。\n",
    "请注意，在求和过程中会触发广播机制\n",
    "（请参阅 :numref:`subsec_broadcasting`）。\n",
    "我们使用sigmoid函数（如 :numref:`sec_mlp`中介绍的）\n",
    "将输入值转换到区间$(0, 1)$。\n",
    "\n",
    "### 候选隐状态\n",
    "\n",
    "接下来，让我们将重置门$\\mathbf{R}_t$\n",
    "与 :eqref:`rnn_h_with_state`\n",
    "中的常规隐状态更新机制集成，\n",
    "得到在时间步$t$的*候选隐状态*（candidate hidden state）\n",
    "$\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$。\n",
    "\n",
    "$$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h),$$\n",
    ":eqlabel:`gru_tilde_H`\n",
    "\n",
    "其中$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$\n",
    "和$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$是权重参数，\n",
    "$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$是偏置项，\n",
    "符号$\\odot$是Hadamard积（按元素乘积）运算符。\n",
    "在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间$(-1, 1)$中。\n",
    "\n",
    "与 :eqref:`rnn_h_with_state`相比，\n",
    " :eqref:`gru_tilde_H`中的$\\mathbf{R}_t$和$\\mathbf{H}_{t-1}$\n",
    "的元素相乘可以减少以往状态的影响。\n",
    "每当重置门$\\mathbf{R}_t$中的项接近$1$时，\n",
    "我们恢复一个如 :eqref:`rnn_h_with_state`中的普通的循环神经网络。\n",
    "对于重置门$\\mathbf{R}_t$中所有接近$0$的项，\n",
    "候选隐状态是以$\\mathbf{X}_t$作为输入的多层感知机的结果。\n",
    "因此，任何预先存在的隐状态都会被*重置*为默认值。\n",
    "\n",
    " :numref:`fig_gru_2`说明了应用重置门之后的计算流程。\n",
    "\n",
    "![在门控循环单元模型中计算候选隐状态](../img/gru-2.svg)\n",
    ":label:`fig_gru_2`\n",
    "\n",
    "### 隐状态\n",
    "\n",
    "上述的计算结果只是候选隐状态，我们仍然需要结合更新门$\\mathbf{Z}_t$的效果。\n",
    "这一步确定新的隐状态$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$\n",
    "在多大程度上来自旧的状态$\\mathbf{H}_{t-1}$和\n",
    "新的候选状态$\\tilde{\\mathbf{H}}_t$。\n",
    "更新门$\\mathbf{Z}_t$仅需要在\n",
    "$\\mathbf{H}_{t-1}$和$\\tilde{\\mathbf{H}}_t$\n",
    "之间进行按元素的凸组合就可以实现这个目标。\n",
    "这就得出了门控循环单元的最终更新公式：\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$\n",
    "\n",
    "每当更新门$\\mathbf{Z}_t$接近$1$时，模型就倾向只保留旧状态。\n",
    "此时，来自$\\mathbf{X}_t$的信息基本上被忽略，\n",
    "从而有效地跳过了依赖链条中的时间步$t$。\n",
    "相反，当$\\mathbf{Z}_t$接近$0$时，\n",
    "新的隐状态$\\mathbf{H}_t$就会接近候选隐状态$\\tilde{\\mathbf{H}}_t$。\n",
    "这些设计可以帮助我们处理循环神经网络中的梯度消失问题，\n",
    "并更好地捕获时间步距离很长的序列的依赖关系。\n",
    "例如，如果整个子序列的所有时间步的更新门都接近于$1$，\n",
    "则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。\n",
    "\n",
    " :numref:`fig_gru_3`说明了更新门起作用后的计算流。\n",
    "\n",
    "![计算门控循环单元模型中的隐状态](../img/gru-3.svg)\n",
    ":label:`fig_gru_3`\n",
    "\n",
    "总之，门控循环单元具有以下两个显著特征：\n",
    "\n",
    "* 重置门有助于捕获序列中的短期依赖关系；\n",
    "* 更新门有助于捕获序列中的长期依赖关系。\n",
    "\n",
    "## 从零开始实现\n",
    "\n",
    "为了更好地理解门控循环单元模型，我们从零开始实现它。\n",
    "首先，我们读取 :numref:`sec_rnn_scratch`中使用的时间机器数据集：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3bbf4ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:25:22.352712Z",
     "iopub.status.busy": "2023-08-18T07:25:22.351930Z",
     "iopub.status.idle": "2023-08-18T07:25:25.503017Z",
     "shell.execute_reply": "2023-08-18T07:25:25.502124Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ..\\data\\timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11663bec",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "### [**初始化模型参数**]\n",
    "\n",
    "下一步是初始化模型参数。\n",
    "我们从标准差为$0.01$的高斯分布中提取权重，\n",
    "并将偏置项设为$0$，超参数`num_hiddens`定义隐藏单元的数量，\n",
    "实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664bb4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:25:25.507494Z",
     "iopub.status.busy": "2023-08-18T07:25:25.506826Z",
     "iopub.status.idle": "2023-08-18T07:25:25.513659Z",
     "shell.execute_reply": "2023-08-18T07:25:25.512868Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                torch.zeros(num_hiddens, device=device))\n",
    "\n",
    "    W_xz, W_hz, b_z = three()  # 更新门参数\n",
    "    W_xr, W_hr, b_r = three()  # 重置门参数\n",
    "    W_xh, W_hh, b_h = three()  # 候选隐状态参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819ab4f",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "### 定义模型\n",
    "\n",
    "现在我们将[**定义隐状态的初始化函数**]`init_gru_state`。\n",
    "与 :numref:`sec_rnn_scratch`中定义的`init_rnn_state`函数一样，\n",
    "此函数返回一个形状为（批量大小，隐藏单元个数）的张量，张量的值全部为零。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77ddd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:25:25.517215Z",
     "iopub.status.busy": "2023-08-18T07:25:25.516638Z",
     "iopub.status.idle": "2023-08-18T07:25:25.520532Z",
     "shell.execute_reply": "2023-08-18T07:25:25.519776Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def init_gru_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb29722",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "现在我们准备[**定义门控循环单元模型**]，\n",
    "模型的架构与基本的循环神经网络单元是相同的，\n",
    "只是权重更新公式更为复杂。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648faa2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:25:25.523984Z",
     "iopub.status.busy": "2023-08-18T07:25:25.523454Z",
     "iopub.status.idle": "2023-08-18T07:25:25.529513Z",
     "shell.execute_reply": "2023-08-18T07:25:25.528547Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def gru(inputs, state, params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)\n",
    "        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)\n",
    "        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = H @ W_hq + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024bb39d",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "### [**训练**]与预测\n",
    "\n",
    "训练和预测的工作方式与 :numref:`sec_rnn_scratch`完全相同。\n",
    "训练结束后，我们分别打印输出训练集的困惑度，\n",
    "以及前缀“time traveler”和“traveler”的预测序列上的困惑度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763946e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:25:25.534734Z",
     "iopub.status.busy": "2023-08-18T07:25:25.534372Z",
     "iopub.status.idle": "2023-08-18T07:29:05.217170Z",
     "shell.execute_reply": "2023-08-18T07:29:05.216294Z"
    },
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()\n",
    "num_epochs, lr = 500, 1\n",
    "model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params,\n",
    "                            init_gru_state, gru)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac9ebd",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "## [**简洁实现**]\n",
    "\n",
    "高级API包含了前文介绍的所有配置细节，\n",
    "所以我们可以直接实例化门控循环单元模型。\n",
    "这段代码的运行速度要快得多，\n",
    "因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549d929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:29:05.220877Z",
     "iopub.status.busy": "2023-08-18T07:29:05.220574Z",
     "iopub.status.idle": "2023-08-18T07:29:30.037933Z",
     "shell.execute_reply": "2023-08-18T07:29:30.036733Z"
    },
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "num_inputs = vocab_size\n",
    "gru_layer = nn.GRU(num_inputs, num_hiddens)\n",
    "model = d2l.RNNModel(gru_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2182df4c",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4449811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "import math\n",
    "\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, num_hiddens, num_layers)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "        \n",
    "    def forward(self, X, state):\n",
    "        #  (batch_size, num_steps)\n",
    "        X = self.embedding(X.T)  # (num_steps, batch_size, embed_size)       \n",
    "        Y, state = self.gru(X, state)       \n",
    "        output = self.dense(Y.reshape(-1, Y.shape[-1]))\n",
    "        return output, state\n",
    "    \n",
    "    def begin_state(self, batch_size, device):\n",
    "        return torch.zeros((self.gru.num_layers, batch_size, self.gru.hidden_size), device=device)\n",
    "\n",
    "\n",
    "vocab_size, embed_size, num_hiddens, num_layers = len(vocab), 100, 256, 2\n",
    "lr, num_epochs, device = 1.0, 500, d2l.try_gpu()\n",
    "\n",
    "model = GRUModel(vocab_size, embed_size, num_hiddens, num_layers).to(device)\n",
    "\n",
    "def train(model, train_iter, vocab, lr, num_epochs, device):  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr)   \n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])    \n",
    "\n",
    "    def predict(prefix, num_preds):\n",
    "        state = model.begin_state(batch_size=1, device=device)\n",
    "        outputs = [vocab[prefix[0]]]\n",
    "        get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))        \n",
    "      \n",
    "        for y in prefix[1:]:\n",
    "            _, state = model(get_input(), state)\n",
    "            outputs.append(vocab[y])        \n",
    "      \n",
    "        for _ in range(num_preds):\n",
    "            y, state = model(get_input(), state)\n",
    "            outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "        \n",
    "        return ''.join([vocab.idx_to_token[i] for i in outputs])    \n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        state = None\n",
    "        metric = d2l.Accumulator(2)  \n",
    "        for X, Y in train_iter:\n",
    "            if state is None:\n",
    "                state = model.begin_state(batch_size=X.shape[0], device=device)\n",
    "            else:\n",
    "                if isinstance(state, tuple):\n",
    "                    state = (s.detach_() for s in state)\n",
    "                else:\n",
    "                    state.detach_()\n",
    "            \n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            y = Y.T.reshape(-1)\n",
    "            y_hat, state = model(X, state)\n",
    "            loss = criterion(y_hat, y.long())            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()     \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()            \n",
    "            metric.add(loss * y.numel(), y.numel())        \n",
    "    \n",
    "        ppl = math.exp(metric[0] / metric[1])       \n",
    "   \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller', 10))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    \n",
    "    print(f'最终困惑度: {ppl:.1f}')\n",
    "    print('完整预测示例:')\n",
    "    print(predict('time traveller', 50))\n",
    "    print(predict('traveller', 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ed80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "required_libs": [],
  "vscode": {
   "interpreter": {
    "hash": "50bfc351199d956b4024fbbd9aca69be4a9c56b71e449cee70b952cf93a37264"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
